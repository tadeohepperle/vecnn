



\begin{table}
\caption{experiment: n=100000, build\_ndc=68256596, model=SliceS2HnswParams, level\_norm\_param=0.3, ef\_construction=40, m\_max=20, m\_max\_0=40, distance=Dot, n\_queries=100}
\label{hnsw_ef_search}
\begin{tabular}{rrrrrrr}
\toprule
rep & build_ms & recall & ndc & time_ms & k & ef \\
\midrule
20 & 25235.543 & 0.857 & 605.370 & 0.278 & 30 & 30 \\
10 & 25235.547 & 0.908 & 733.280 & 0.343 & 30 & 40 \\
10 & 25235.547 & 0.917 & 834.890 & 0.397 & 30 & 50 \\
10 & 25235.547 & 0.932 & 944.840 & 0.455 & 30 & 60 \\
10 & 25235.547 & 0.937 & 1043.480 & 0.508 & 30 & 70 \\
10 & 25235.547 & 0.941 & 1136.230 & 0.558 & 30 & 80 \\
10 & 25235.547 & 0.947 & 1236.740 & 0.613 & 30 & 90 \\
10 & 25235.547 & 0.951 & 1331.650 & 0.663 & 30 & 100 \\
10 & 25235.547 & 0.954 & 1427.740 & 0.716 & 30 & 110 \\
10 & 25235.547 & 0.956 & 1520.500 & 0.764 & 30 & 120 \\
10 & 25235.547 & 0.966 & 1631.420 & 0.829 & 30 & 130 \\
10 & 25235.547 & 0.966 & 1722.670 & 0.878 & 30 & 140 \\
10 & 25235.547 & 0.968 & 1817.980 & 0.931 & 30 & 150 \\
10 & 25235.547 & 0.969 & 1906.980 & 0.979 & 30 & 160 \\
10 & 25235.547 & 0.969 & 1995.340 & 1.030 & 30 & 170 \\
10 & 25235.547 & 0.972 & 2082.470 & 1.080 & 30 & 180 \\
10 & 25235.547 & 0.973 & 2168.070 & 1.126 & 30 & 190 \\
10 & 25235.547 & 0.973 & 2257.250 & 1.177 & 30 & 200 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=5, n=100000, build\_ndc=176267066, build\_ms=35087.152, model=RNNGraphParams, outer\_loops=3, inner\_loops=4, max\_neighbors\_after\_reverse\_pruning=40, initial\_neighbors=40, distance=Dot, dist=Dot, k=30, ef=60}
\label{exp_rnn_effect_of_start_candidates}
\begin{tabular}{rrrr}
\toprule
recall & ndc & time_ms & start_candidates \\
\midrule
0.964 & 1395.455 & 0.533 & 1 \\
0.964 & 1366.731 & 0.523 & 2 \\
0.964 & 1363.300 & 0.523 & 4 \\
0.964 & 1352.325 & 0.518 & 8 \\
0.964 & 1348.970 & 0.517 & 16 \\
0.964 & 1366.765 & 0.520 & 32 \\
0.967 & 1452.589 & 0.550 & 64 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, level\_norm\_param=0.3, ef\_construction=60, m\_max=20, m\_max\_0=40, distance=Dot, dist=Dot, k=30, ef=60, start\_candidates=1, n\_queries=1000}
\label{hnsw_effect_of_n}
\begin{tabular}{rrrrlr}
\toprule
n & build_ms & recall & time_ms & model & build_ms_per_n \\
\midrule
10000 & 1548.237 & 0.975 & 0.198 & SliceS2HnswParams & 0.155 \\
10000 & 1560.561 & 0.975 & 0.198 & SliceS2HnswParams & 0.156 \\
30000 & 6609.136 & 0.963 & 0.317 & SliceS2HnswParams & 0.220 \\
30000 & 6628.716 & 0.963 & 0.321 & SliceS2HnswParams & 0.221 \\
100000 & 32093.129 & 0.954 & 0.418 & SliceS2HnswParams & 0.321 \\
300000 & 121519.670 & 0.926 & 0.475 & SliceS2HnswParams & 0.405 \\
1000000 & 472444.100 & 0.910 & 0.531 & SliceS2HnswParams & 0.472 \\
3000000 & 1773315.000 & 0.891 & 0.652 & SliceS2HnswParams & 0.591 \\
10000000 & 6847348.500 & 0.860 & 0.764 & SliceS2HnswParams & 0.685 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=10, n=100000, model=SliceS2HnswParams, level\_norm\_param=0.3, ef\_construction=40, distance=Dot, k=30, ef=60, n\_queries=100}
\label{exp_hnsw_effect_of_m_max}
\begin{tabular}{rrrrrrr}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & m_max & m_max_0 \\
\midrule
36808102 & 13249.221 & 0.780 & 499.520 & 0.245 & 8 & 16 \\
48070390 & 17453.629 & 0.870 & 648.240 & 0.313 & 12 & 24 \\
58388815 & 21242.572 & 0.921 & 796.530 & 0.382 & 16 & 32 \\
68256596 & 25070.785 & 0.932 & 944.840 & 0.455 & 20 & 40 \\
77681703 & 28861.855 & 0.952 & 1073.430 & 0.517 & 24 & 48 \\
86781248 & 32408.568 & 0.967 & 1207.920 & 0.584 & 28 & 56 \\
95641999 & 35788.406 & 0.973 & 1337.820 & 0.644 & 32 & 64 \\
104292049 & 39173.336 & 0.976 & 1447.430 & 0.698 & 36 & 72 \\
112772348 & 42454.195 & 0.978 & 1557.000 & 0.749 & 40 & 80 \\
119418513 & 44997.336 & 0.980 & 1621.790 & 0.781 & 44 & 88 \\
125592102 & 47446.934 & 0.983 & 1706.110 & 0.819 & 48 & 96 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=SliceS2HnswParams, level\_norm\_param=0.3, m\_max=20, m\_max\_0=40, distance=Dot, k=30, ef=60, n\_queries=1000}
\label{exp_hnsw_effect_of_ef_construction}
\begin{tabular}{rrrrrr}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & ef_construction \\
\midrule
57288264 & 21519.887 & 0.934 & 945.833 & 0.454 & 30 \\
68256596 & 26567.621 & 0.942 & 951.465 & 0.454 & 40 \\
78697221 & 31643.450 & 0.949 & 954.864 & 0.456 & 50 \\
88751642 & 36373.715 & 0.954 & 954.500 & 0.457 & 60 \\
98509666 & 41057.207 & 0.956 & 955.501 & 0.457 & 70 \\
108029599 & 45691.520 & 0.955 & 956.396 & 0.458 & 80 \\
117317580 & 50133.516 & 0.954 & 959.150 & 0.458 & 90 \\
126404169 & 54606.316 & 0.955 & 956.667 & 0.457 & 100 \\
135342504 & 58985.200 & 0.954 & 956.864 & 0.457 & 110 \\
144123590 & 63431.020 & 0.954 & 958.383 & 0.458 & 120 \\
152761979 & 68265.670 & 0.955 & 958.824 & 0.458 & 130 \\
161274875 & 73649.160 & 0.955 & 960.158 & 0.458 & 140 \\
169652552 & 78147.810 & 0.956 & 959.992 & 0.459 & 150 \\
177922105 & 82852.970 & 0.956 & 959.988 & 0.459 & 160 \\
186068823 & 87205.690 & 0.956 & 960.456 & 0.459 & 170 \\
194151974 & 92060.836 & 0.956 & 959.826 & 0.459 & 180 \\
202099637 & 96275.160 & 0.957 & 960.128 & 0.459 & 190 \\
209973660 & 101291.830 & 0.957 & 959.952 & 0.459 & 200 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=SliceS2HnswParams, ef\_construction=40, m\_max=20, m\_max\_0=40, distance=Dot, k=30, ef=60, n\_queries=1000}
\label{exp_hnsw_effect_of_level_norm}
\begin{tabular}{rrrrrr}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & level_norm_param \\
\midrule
71480325 & 27947.188 & 0.946 & 1005.852 & 0.483 & 0.000 \\
71480325 & 27978.068 & 0.946 & 1005.852 & 0.484 & 0.050 \\
69095268 & 26918.863 & 0.948 & 956.369 & 0.465 & 0.100 \\
67409624 & 26807.525 & 0.945 & 956.116 & 0.463 & 0.150 \\
67737278 & 26632.697 & 0.941 & 947.586 & 0.457 & 0.200 \\
68486997 & 26925.525 & 0.944 & 960.102 & 0.460 & 0.250 \\
68256596 & 26486.280 & 0.942 & 951.465 & 0.454 & 0.300 \\
70264320 & 27294.887 & 0.945 & 967.274 & 0.460 & 0.350 \\
71547316 & 27601.082 & 0.941 & 973.157 & 0.462 & 0.400 \\
72252087 & 27786.744 & 0.944 & 964.004 & 0.455 & 0.450 \\
74464445 & 28346.994 & 0.943 & 978.981 & 0.458 & 0.500 \\
76074030 & 28696.996 & 0.948 & 994.594 & 0.464 & 0.550 \\
78125994 & 29509.424 & 0.945 & 1003.657 & 0.464 & 0.600 \\
79980745 & 30024.832 & 0.945 & 1010.975 & 0.467 & 0.650 \\
82141055 & 30580.764 & 0.946 & 1022.250 & 0.468 & 0.700 \\
83914803 & 31170.762 & 0.943 & 1024.223 & 0.466 & 0.750 \\
86115446 & 31834.549 & 0.943 & 1038.131 & 0.469 & 0.800 \\
88132095 & 32220.360 & 0.945 & 1046.826 & 0.471 & 0.850 \\
90455238 & 33012.125 & 0.942 & 1056.526 & 0.472 & 0.900 \\
92437024 & 33668.700 & 0.945 & 1063.898 & 0.473 & 0.950 \\
94722962 & 34345.074 & 0.944 & 1073.462 & 0.475 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: n=100000, build\_ndc=68256596, model=SliceS2HnswParams, level\_norm\_param=0.3, ef\_construction=40, m\_max=20, m\_max\_0=40, distance=Dot, n\_queries=100}
\label{hnsw_k}
\begin{tabular}{rrrrrrr}
\toprule
rep & build_ms & recall & ndc & time_ms & k & ef \\
\midrule
20 & 25235.543 & 0.857 & 605.370 & 0.278 & 30 & 30 \\
10 & 25235.547 & 0.895 & 733.280 & 0.342 & 40 & 40 \\
10 & 25235.547 & 0.898 & 834.890 & 0.396 & 50 & 50 \\
10 & 25235.547 & 0.911 & 944.840 & 0.454 & 60 & 60 \\
10 & 25235.547 & 0.915 & 1043.480 & 0.507 & 70 & 70 \\
10 & 25235.547 & 0.915 & 1136.230 & 0.558 & 80 & 80 \\
10 & 25235.547 & 0.919 & 1236.740 & 0.610 & 90 & 90 \\
10 & 25235.547 & 0.920 & 1331.650 & 0.660 & 100 & 100 \\
10 & 25235.547 & 0.923 & 1427.740 & 0.712 & 110 & 110 \\
10 & 25235.547 & 0.924 & 1520.500 & 0.761 & 120 & 120 \\
10 & 25235.547 & 0.934 & 1631.420 & 0.823 & 130 & 130 \\
10 & 25235.547 & 0.936 & 1722.670 & 0.873 & 140 & 140 \\
10 & 25235.547 & 0.937 & 1817.980 & 0.925 & 150 & 150 \\
10 & 25235.547 & 0.939 & 1906.980 & 0.974 & 160 & 160 \\
10 & 25235.547 & 0.940 & 1995.340 & 1.026 & 170 & 170 \\
10 & 25235.547 & 0.941 & 2082.470 & 1.074 & 180 & 180 \\
10 & 25235.547 & 0.940 & 2168.070 & 1.122 & 190 & 190 \\
10 & 25235.547 & 0.940 & 2257.250 & 1.171 & 200 & 200 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: n=100000, build\_ndc=176353688, model=RNNGraphParams, outer\_loops=3, inner\_loops=4, max\_neighbors\_after\_reverse\_pruning=40, initial\_neighbors=40, distance=Dot}
\label{rnn_ef_search}
\begin{tabular}{rrrrrlrrrr}
\toprule
rep & build_ms & recall & ndc & time_ms & dist & k & ef & start_candidates & n_queries \\
\midrule
5 & 35608.100 & 0.963 & 1391.811 & 0.550 & Dot & 30 & 60 & 1.000 & 1000 \\
5 & 35608.100 & 0.964 & 1387.460 & 0.545 & Dot & 30 & 60 & 2.000 & 1000 \\
5 & 35608.100 & 0.964 & 1404.381 & 0.547 & Dot & 30 & 60 & 3.000 & 1000 \\
5 & 35608.100 & 0.964 & 1404.859 & 0.547 & Dot & 30 & 60 & 4.000 & 1000 \\
5 & 35608.100 & 0.964 & 1403.975 & 0.546 & Dot & 30 & 60 & 5.000 & 1000 \\
5 & 35608.100 & 0.964 & 1387.272 & 0.542 & Dot & 30 & 60 & 6.000 & 1000 \\
5 & 35608.100 & 0.964 & 1384.855 & 0.542 & Dot & 30 & 60 & 7.000 & 1000 \\
5 & 35608.100 & 0.964 & 1378.623 & 0.539 & Dot & 30 & 60 & 8.000 & 1000 \\
5 & 35608.100 & 0.964 & 1379.375 & 0.540 & Dot & 30 & 60 & 9.000 & 1000 \\
5 & 35608.100 & 0.964 & 1379.338 & 0.539 & Dot & 30 & 60 & 10.000 & 1000 \\
5 & 35608.100 & 0.964 & 1375.024 & 0.538 & Dot & 30 & 60 & 11.000 & 1000 \\
5 & 35608.100 & 0.964 & 1370.499 & 0.536 & Dot & 30 & 60 & 12.000 & 1000 \\
5 & 35608.100 & 0.964 & 1370.277 & 0.536 & Dot & 30 & 60 & 13.000 & 1000 \\
5 & 35608.100 & 0.964 & 1366.488 & 0.535 & Dot & 30 & 60 & 14.000 & 1000 \\
5 & 35608.100 & 0.964 & 1367.336 & 0.535 & Dot & 30 & 60 & 15.000 & 1000 \\
5 & 35608.100 & 0.964 & 1367.352 & 0.535 & Dot & 30 & 60 & 16.000 & 1000 \\
5 & 35608.100 & 0.964 & 1366.740 & 0.535 & Dot & 30 & 60 & 17.000 & 1000 \\
5 & 35608.100 & 0.964 & 1385.852 & 0.539 & Dot & 30 & 60 & 18.000 & 1000 \\
5 & 35608.100 & 0.964 & 1386.760 & 0.539 & Dot & 30 & 60 & 19.000 & 1000 \\
5 & 35608.100 & 0.964 & 1387.261 & 0.539 & Dot & 30 & 60 & 20.000 & 1000 \\
10 & 35583.867 & 0.908 & 823.330 & 0.357 & NaN & 30 & 30 & NaN & 100 \\
5 & 35583.867 & 0.932 & 1001.280 & 0.390 & NaN & 30 & 40 & NaN & 100 \\
5 & 35583.867 & 0.947 & 1195.440 & 0.470 & NaN & 30 & 50 & NaN & 100 \\
5 & 35583.867 & 0.960 & 1389.810 & 0.545 & NaN & 30 & 60 & NaN & 100 \\
5 & 35583.867 & 0.972 & 1562.620 & 0.617 & NaN & 30 & 70 & NaN & 100 \\
5 & 35583.867 & 0.976 & 1728.240 & 0.685 & NaN & 30 & 80 & NaN & 100 \\
5 & 35583.867 & 0.979 & 1897.290 & 0.755 & NaN & 30 & 90 & NaN & 100 \\
5 & 35583.867 & 0.983 & 2071.280 & 0.825 & NaN & 30 & 100 & NaN & 100 \\
5 & 35583.867 & 0.986 & 2244.950 & 0.896 & NaN & 30 & 110 & NaN & 100 \\
5 & 35583.867 & 0.988 & 2400.920 & 0.959 & NaN & 30 & 120 & NaN & 100 \\
5 & 35583.867 & 0.988 & 2554.860 & 1.023 & NaN & 30 & 130 & NaN & 100 \\
5 & 35583.867 & 0.990 & 2717.120 & 1.092 & NaN & 30 & 140 & NaN & 100 \\
5 & 35583.867 & 0.992 & 2865.040 & 1.154 & NaN & 30 & 150 & NaN & 100 \\
5 & 35583.867 & 0.993 & 3034.470 & 1.225 & NaN & 30 & 160 & NaN & 100 \\
5 & 35583.867 & 0.994 & 3200.410 & 1.293 & NaN & 30 & 170 & NaN & 100 \\
5 & 35583.867 & 0.994 & 3343.150 & 1.357 & NaN & 30 & 180 & NaN & 100 \\
5 & 35583.867 & 0.995 & 3496.310 & 1.422 & NaN & 30 & 190 & NaN & 100 \\
5 & 35583.867 & 0.995 & 3643.700 & 1.485 & NaN & 30 & 200 & NaN & 100 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: n=100000, build\_ndc=176353688, model=RNNGraphParams, outer\_loops=3, inner\_loops=4, max\_neighbors\_after\_reverse\_pruning=40, initial\_neighbors=40, distance=Dot}
\label{rnn_k}
\begin{tabular}{rrrrrlrrrr}
\toprule
rep & build_ms & recall & ndc & time_ms & dist & k & ef & start_candidates & n_queries \\
\midrule
10 & 35583.867 & 0.908 & 823.330 & 0.357 & NaN & 30 & 30 & NaN & 100 \\
5 & 35583.867 & 0.915 & 1001.280 & 0.389 & NaN & 40 & 40 & NaN & 100 \\
5 & 35583.867 & 0.921 & 1195.440 & 0.469 & NaN & 50 & 50 & NaN & 100 \\
5 & 35583.867 & 0.931 & 1389.810 & 0.544 & NaN & 60 & 60 & NaN & 100 \\
5 & 35583.867 & 0.937 & 1562.620 & 0.616 & NaN & 70 & 70 & NaN & 100 \\
5 & 35583.867 & 0.939 & 1728.240 & 0.683 & NaN & 80 & 80 & NaN & 100 \\
5 & 35583.867 & 0.939 & 1897.290 & 0.754 & NaN & 90 & 90 & NaN & 100 \\
5 & 35583.867 & 0.943 & 2071.280 & 0.829 & NaN & 100 & 100 & NaN & 100 \\
5 & 35583.867 & 0.948 & 2244.950 & 0.895 & NaN & 110 & 110 & NaN & 100 \\
5 & 35583.867 & 0.952 & 2400.920 & 0.958 & NaN & 120 & 120 & NaN & 100 \\
5 & 35583.867 & 0.953 & 2554.860 & 1.022 & NaN & 130 & 130 & NaN & 100 \\
5 & 35583.867 & 0.955 & 2717.120 & 1.091 & NaN & 140 & 140 & NaN & 100 \\
5 & 35583.867 & 0.956 & 2865.040 & 1.154 & NaN & 150 & 150 & NaN & 100 \\
5 & 35583.867 & 0.956 & 3034.470 & 1.224 & NaN & 160 & 160 & NaN & 100 \\
5 & 35583.867 & 0.958 & 3200.410 & 1.294 & NaN & 170 & 170 & NaN & 100 \\
5 & 35583.867 & 0.959 & 3343.150 & 1.357 & NaN & 180 & 180 & NaN & 100 \\
5 & 35583.867 & 0.960 & 3496.310 & 1.421 & NaN & 190 & 190 & NaN & 100 \\
5 & 35583.867 & 0.961 & 3643.700 & 1.485 & NaN & 200 & 200 & NaN & 100 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=RNNGraphParams, outer\_loops=1, max\_neighbors\_after\_reverse\_pruning=40, initial\_neighbors=40, distance=Dot, k=30, ef=60, n\_queries=10000}
\label{rnn_inner_loops}
\begin{tabular}{rrrrrr}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & inner_loops \\
\midrule
18093216 & 5020.736 & 0.182 & 6890.852 & 2.561 & 1 \\
47360250 & 10345.127 & 0.498 & 6034.579 & 2.273 & 2 \\
84119168 & 16507.530 & 0.787 & 5115.407 & 1.929 & 3 \\
121094600 & 22545.460 & 0.902 & 4362.885 & 1.660 & 4 \\
153637297 & 27923.172 & 0.936 & 3589.844 & 1.374 & 5 \\
178045375 & 32049.865 & 0.946 & 2997.026 & 1.153 & 6 \\
193790275 & 34962.348 & 0.948 & 2541.850 & 0.978 & 7 \\
202938930 & 36881.090 & 0.946 & 2225.653 & 0.858 & 8 \\
207904596 & 38107.043 & 0.943 & 2025.744 & 0.781 & 9 \\
210504295 & 38888.445 & 0.941 & 1924.066 & 0.740 & 10 \\
211870880 & 39362.184 & 0.940 & 1898.839 & 0.730 & 11 \\
212615549 & 39702.180 & 0.939 & 1859.634 & 0.715 & 12 \\
213060685 & 39920.290 & 0.939 & 1840.141 & 0.705 & 13 \\
213376286 & 40097.645 & 0.938 & 1839.188 & 0.705 & 14 \\
213615193 & 40276.246 & 0.938 & 1841.292 & 0.705 & 15 \\
213823209 & 40407.457 & 0.938 & 1831.368 & 0.699 & 16 \\
214023428 & 40556.465 & 0.938 & 1835.402 & 0.702 & 17 \\
214214488 & 40692.906 & 0.938 & 1837.908 & 0.703 & 18 \\
214398580 & 40798.285 & 0.938 & 1830.072 & 0.699 & 19 \\
214590119 & 40932.210 & 0.938 & 1836.777 & 0.702 & 20 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: n=100000, model=RNNGraphParams, max\_neighbors\_after\_reverse\_pruning=40, initial\_neighbors=40, distance=Dot, k=30, ef=60, n\_queries=10000}
\label{rnn_outer_loops}
\begin{tabular}{rrrrrrrr}
\toprule
rep & build_ndc & build_ms & recall & ndc & time_ms & outer_loops & inner_loops \\
\midrule
1 & 213093231 & 39571.600 & 0.941 & 1881.688 & 0.700 & 1 & 12 \\
1 & 209811439 & 39349.125 & 0.959 & 1288.130 & 0.488 & 2 & 6 \\
1 & 176267066 & 35072.957 & 0.964 & 1395.455 & 0.533 & 3 & 4 \\
1 & 160478802 & 33414.586 & 0.965 & 1407.369 & 0.537 & 4 & 3 \\
1 & 136868577 & 30656.451 & 0.964 & 1406.804 & 0.537 & 6 & 2 \\
1 & 107326138 & 27886.764 & 0.945 & 1268.439 & 0.486 & 12 & 1 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=StitchingParams, max\_chunk\_size=256, same\_chunk\_m\_max=40, keep\_fraction=0.0, m\_max=40, x\_or\_ef=10, only\_n\_chunks=None, distance=Dot, n\_candidates=0, k=30, ef=60, n\_queries=10000}
\label{exp_stitching_effect_of_fraction}
\begin{tabular}{rrrrrrl}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & neg_fraction & stitch_mode \\
\midrule
11196114 & 2059.709 & 0.006 & 182.543 & 0.129 & 0.000 & method 1 \\
11196114 & 2058.291 & 0.006 & 182.543 & 0.129 & 0.000 & method 2 \\
11196114 & 2059.000 & 0.006 & 182.543 & 0.129 & 0.000 & method 3 \\
11196114 & 2059.195 & 0.006 & 182.543 & 0.129 & 0.000 & method 4 \\
13286326 & 2541.221 & 0.025 & 422.430 & 0.251 & 0.050 & method 1 \\
14539469 & 2928.280 & 0.180 & 922.800 & 0.546 & 0.050 & method 2 \\
13699633 & 2636.737 & 0.216 & 940.067 & 0.577 & 0.050 & method 3 \\
25329423 & 6123.817 & 0.672 & 1392.900 & 0.838 & 0.050 & method 4 \\
15136942 & 2943.582 & 0.026 & 427.017 & 0.254 & 0.100 & method 1 \\
18220215 & 3877.919 & 0.347 & 1225.364 & 0.730 & 0.100 & method 2 \\
17061085 & 3376.032 & 0.390 & 1269.672 & 0.761 & 0.100 & method 3 \\
39282774 & 10032.052 & 0.752 & 1352.386 & 0.822 & 0.100 & method 4 \\
16952509 & 3346.692 & 0.026 & 428.062 & 0.256 & 0.150 & method 1 \\
22102331 & 4872.144 & 0.464 & 1382.417 & 0.824 & 0.150 & method 2 \\
20551614 & 4153.532 & 0.491 & 1405.056 & 0.841 & 0.150 & method 3 \\
52650459 & 13831.483 & 0.778 & 1319.932 & 0.800 & 0.150 & method 4 \\
18777848 & 3739.382 & 0.026 & 428.398 & 0.263 & 0.200 & method 1 \\
26174304 & 5926.481 & 0.550 & 1479.148 & 0.882 & 0.200 & method 2 \\
24232195 & 4965.305 & 0.563 & 1483.743 & 0.888 & 0.200 & method 3 \\
65987791 & 17508.828 & 0.798 & 1294.065 & 0.787 & 0.200 & method 4 \\
20572033 & 4139.880 & 0.026 & 428.523 & 0.255 & 0.250 & method 1 \\
30276308 & 6977.962 & 0.610 & 1545.786 & 0.922 & 0.250 & method 2 \\
28003204 & 5802.026 & 0.616 & 1543.729 & 0.922 & 0.250 & method 3 \\
79071142 & 21109.860 & 0.808 & 1275.196 & 0.781 & 0.250 & method 4 \\
22374149 & 4529.351 & 0.026 & 428.618 & 0.254 & 0.300 & method 1 \\
34572018 & 8031.538 & 0.659 & 1594.078 & 0.954 & 0.300 & method 2 \\
31755999 & 6586.140 & 0.653 & 1584.673 & 0.951 & 0.300 & method 3 \\
92200215 & 24861.982 & 0.813 & 1261.235 & 0.773 & 0.300 & method 4 \\
24179432 & 4876.477 & 0.026 & 429.147 & 0.255 & 0.350 & method 1 \\
38939931 & 9150.679 & 0.695 & 1627.809 & 0.975 & 0.350 & method 2 \\
35649094 & 7448.555 & 0.688 & 1610.866 & 0.965 & 0.350 & method 3 \\
105215442 & 28705.023 & 0.819 & 1256.487 & 0.771 & 0.350 & method 4 \\
25995450 & 5272.060 & 0.026 & 429.366 & 0.255 & 0.400 & method 1 \\
43388618 & 10289.668 & 0.728 & 1653.072 & 1.005 & 0.400 & method 2 \\
39657926 & 8363.935 & 0.712 & 1626.949 & 0.971 & 0.400 & method 3 \\
118332069 & 32010.830 & 0.825 & 1244.191 & 0.772 & 0.400 & method 4 \\
27785904 & 5715.355 & 0.026 & 429.367 & 0.255 & 0.450 & method 1 \\
47842501 & 11444.337 & 0.749 & 1669.954 & 1.003 & 0.450 & method 2 \\
43628240 & 9221.869 & 0.733 & 1642.032 & 0.985 & 0.450 & method 3 \\
131183106 & 35544.880 & 0.828 & 1235.782 & 0.759 & 0.450 & method 4 \\
29601400 & 6051.013 & 0.026 & 429.379 & 0.255 & 0.500 & method 1 \\
52398801 & 12602.765 & 0.770 & 1679.082 & 1.013 & 0.500 & method 2 \\
47669993 & 10101.219 & 0.750 & 1655.328 & 0.994 & 0.500 & method 3 \\
144288046 & 39260.690 & 0.833 & 1226.100 & 0.755 & 0.500 & method 4 \\
31401305 & 6506.249 & 0.026 & 429.379 & 0.255 & 0.550 & method 1 \\
56988194 & 13891.705 & 0.788 & 1690.359 & 1.016 & 0.550 & method 2 \\
51539911 & 11065.573 & 0.764 & 1667.548 & 1.009 & 0.550 & method 3 \\
157265662 & 42953.055 & 0.834 & 1223.038 & 0.752 & 0.550 & method 4 \\
33202399 & 6961.467 & 0.026 & 429.380 & 0.254 & 0.600 & method 1 \\
61626504 & 15082.755 & 0.803 & 1700.245 & 1.021 & 0.600 & method 2 \\
55662491 & 11973.899 & 0.778 & 1676.806 & 1.007 & 0.600 & method 3 \\
170281915 & 46591.703 & 0.837 & 1221.012 & 0.751 & 0.600 & method 4 \\
35008220 & 7298.609 & 0.026 & 429.380 & 0.254 & 0.650 & method 1 \\
66242082 & 16402.816 & 0.814 & 1711.211 & 1.028 & 0.650 & method 2 \\
59792751 & 12898.453 & 0.789 & 1683.710 & 1.010 & 0.650 & method 3 \\
183274830 & 50171.258 & 0.843 & 1210.310 & 0.745 & 0.650 & method 4 \\
36809884 & 7687.468 & 0.026 & 429.384 & 0.254 & 0.700 & method 1 \\
70961719 & 17511.404 & 0.825 & 1717.209 & 1.033 & 0.700 & method 2 \\
63952407 & 13824.126 & 0.799 & 1687.225 & 1.013 & 0.700 & method 3 \\
196302299 & 53791.730 & 0.842 & 1203.843 & 0.742 & 0.700 & method 4 \\
38622651 & 8081.647 & 0.026 & 429.384 & 0.255 & 0.750 & method 1 \\
75646796 & 18720.010 & 0.835 & 1719.793 & 1.034 & 0.750 & method 2 \\
68138209 & 14756.530 & 0.809 & 1693.168 & 1.018 & 0.750 & method 3 \\
209246622 & 57485.004 & 0.845 & 1202.052 & 0.742 & 0.750 & method 4 \\
40416544 & 8619.475 & 0.026 & 429.384 & 0.255 & 0.800 & method 1 \\
80361984 & 19935.398 & 0.843 & 1727.289 & 1.040 & 0.800 & method 2 \\
72305557 & 15691.737 & 0.817 & 1700.589 & 1.047 & 0.800 & method 3 \\
222246008 & 61422.400 & 0.844 & 1199.255 & 0.741 & 0.800 & method 4 \\
42222562 & 8868.227 & 0.026 & 429.384 & 0.255 & 0.850 & method 1 \\
85071939 & 21153.574 & 0.851 & 1729.321 & 1.042 & 0.850 & method 2 \\
76491053 & 16751.150 & 0.825 & 1702.264 & 1.024 & 0.850 & method 3 \\
235140653 & 64815.980 & 0.846 & 1197.932 & 0.740 & 0.850 & method 4 \\
44032213 & 9266.774 & 0.026 & 429.384 & 0.255 & 0.900 & method 1 \\
89825401 & 22433.307 & 0.857 & 1730.213 & 1.042 & 0.900 & method 2 \\
80708084 & 17558.717 & 0.831 & 1706.613 & 1.027 & 0.900 & method 3 \\
248219234 & 68563.125 & 0.848 & 1189.628 & 0.742 & 0.900 & method 4 \\
45843503 & 9660.867 & 0.026 & 429.384 & 0.255 & 0.950 & method 1 \\
94585155 & 23589.209 & 0.863 & 1734.857 & 1.045 & 0.950 & method 2 \\
84894082 & 18562.076 & 0.838 & 1711.230 & 1.030 & 0.950 & method 3 \\
261211341 & 72141.890 & 0.849 & 1188.474 & 0.734 & 0.950 & method 4 \\
47665058 & 10051.133 & 0.026 & 429.384 & 0.255 & 1.000 & method 1 \\
99388106 & 24951.094 & 0.869 & 1733.293 & 1.045 & 1.000 & method 2 \\
89166159 & 19460.260 & 0.845 & 1711.965 & 1.031 & 1.000 & method 3 \\
274312021 & 75819.660 & 0.849 & 1184.522 & 0.732 & 1.000 & method 4 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=StitchingParams, same\_chunk\_m\_max=40, keep\_fraction=0.0, m\_max=40, x\_or\_ef=8, only\_n\_chunks=None, distance=Dot, n\_candidates=0, k=30, ef=60, n\_queries=10000}
\label{exp_stitching_effect_of_max_chunk_size}
\begin{tabular}{rrrrrrrl}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & max_chunk_size & neg_fraction & stitch_mode \\
\midrule
52821861 & 13175.962 & 0.764 & 1765.603 & 1.039 & 1 & 0.600 & method 2 \\
49724950 & 10879.275 & 0.745 & 1739.576 & 1.024 & 1 & 0.600 & method 3 \\
56711220 & 15212.481 & 0.791 & 1346.644 & 0.810 & 1 & 0.200 & method 4 \\
57456524 & 14167.727 & 0.784 & 1748.537 & 1.031 & 2 & 0.600 & method 2 \\
52772763 & 11515.407 & 0.765 & 1730.617 & 1.024 & 2 & 0.600 & method 3 \\
57268707 & 15136.979 & 0.793 & 1350.866 & 0.813 & 2 & 0.200 & method 4 \\
61627131 & 14957.009 & 0.803 & 1700.353 & 1.006 & 3 & 0.600 & method 2 \\
56021787 & 12261.138 & 0.782 & 1678.125 & 0.992 & 3 & 0.600 & method 3 \\
59339467 & 15399.037 & 0.804 & 1337.914 & 0.807 & 3 & 0.200 & method 4 \\
68966090 & 16117.332 & 0.814 & 1616.923 & 0.990 & 4 & 0.600 & method 2 \\
63291468 & 13298.792 & 0.797 & 1609.642 & 0.958 & 4 & 0.600 & method 3 \\
65573458 & 16168.901 & 0.811 & 1315.838 & 0.796 & 4 & 0.200 & method 4 \\
84822502 & 18756.443 & 0.826 & 1502.141 & 0.902 & 5 & 0.600 & method 2 \\
79420388 & 15775.866 & 0.809 & 1486.140 & 0.892 & 5 & 0.600 & method 3 \\
80907835 & 18482.172 & 0.826 & 1217.750 & 0.746 & 5 & 0.200 & method 4 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=StitchingParams, max\_chunk\_size=256, same\_chunk\_m\_max=40, keep\_fraction=0.0, x\_or\_ef=8, only\_n\_chunks=None, distance=Dot, n\_candidates=0, k=30, ef=60, n\_queries=10000}
\label{exp_stitching_effect_of_m_max}
\begin{tabular}{rrrrrrrl}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & neg_fraction & m_max & stitch_mode \\
\midrule
16281319 & 3140.693 & 0.050 & 150.522 & 0.081 & 0.600 & 4 & method 2 \\
17857361 & 3117.007 & 0.073 & 166.731 & 0.091 & 0.600 & 4 & method 3 \\
21419122 & 4443.298 & 0.024 & 125.061 & 0.067 & 0.200 & 4 & method 4 \\
22403073 & 4666.708 & 0.325 & 399.140 & 0.232 & 0.600 & 8 & method 2 \\
22939989 & 4254.069 & 0.391 & 419.111 & 0.245 & 0.600 & 8 & method 3 \\
26931251 & 5873.580 & 0.114 & 257.991 & 0.141 & 0.200 & 8 & method 4 \\
28376166 & 6114.954 & 0.548 & 621.060 & 0.367 & 0.600 & 12 & method 2 \\
27847335 & 5382.862 & 0.579 & 622.211 & 0.369 & 0.600 & 12 & method 3 \\
31773138 & 7276.127 & 0.300 & 436.385 & 0.248 & 0.200 & 12 & method 4 \\
33999212 & 7578.142 & 0.655 & 793.813 & 0.475 & 0.600 & 16 & method 2 \\
32446852 & 6461.838 & 0.659 & 792.798 & 0.475 & 0.600 & 16 & method 3 \\
36204808 & 8484.191 & 0.453 & 602.366 & 0.350 & 0.200 & 16 & method 4 \\
39198503 & 8920.228 & 0.709 & 953.718 & 0.570 & 0.600 & 20 & method 2 \\
36788545 & 7497.188 & 0.707 & 958.654 & 0.573 & 0.600 & 20 & method 3 \\
40394937 & 9701.719 & 0.575 & 745.313 & 0.440 & 0.200 & 20 & method 4 \\
44110116 & 10204.424 & 0.744 & 1110.562 & 0.662 & 0.600 & 24 & method 2 \\
40910831 & 8459.356 & 0.737 & 1107.286 & 0.661 & 0.600 & 24 & method 3 \\
44430532 & 10912.741 & 0.655 & 873.659 & 0.521 & 0.200 & 24 & method 4 \\
48757943 & 11427.762 & 0.767 & 1269.099 & 0.756 & 0.600 & 28 & method 2 \\
44882671 & 9397.179 & 0.754 & 1252.775 & 0.746 & 0.600 & 28 & method 3 \\
48289969 & 11973.310 & 0.712 & 996.178 & 0.612 & 0.200 & 28 & method 4 \\
53200953 & 12705.124 & 0.784 & 1415.357 & 0.844 & 0.600 & 32 & method 2 \\
48708117 & 10389.642 & 0.765 & 1395.779 & 0.833 & 0.600 & 32 & method 3 \\
52028170 & 13100.723 & 0.751 & 1114.504 & 0.688 & 0.200 & 32 & method 4 \\
57488551 & 13812.674 & 0.794 & 1563.376 & 0.928 & 0.600 & 36 & method 2 \\
52392322 & 11089.015 & 0.774 & 1539.739 & 0.914 & 0.600 & 36 & method 3 \\
55735091 & 14169.217 & 0.783 & 1230.704 & 0.757 & 0.200 & 36 & method 4 \\
61626815 & 14852.677 & 0.803 & 1700.358 & 1.010 & 0.600 & 40 & method 2 \\
56021567 & 12043.133 & 0.782 & 1678.062 & 0.996 & 0.600 & 40 & method 3 \\
59335665 & 15409.117 & 0.804 & 1337.572 & 0.809 & 0.200 & 40 & method 4 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=EnsembleParams, max\_chunk\_size=256, same\_chunk\_m\_max=16, m\_max=20, m\_max\_0=40, level\_norm=0.0, distance=Dot, strategy=BruteForceKNN, n\_candidates=0, k=30, ef=60, n\_queries=1000}
\label{exp_ensemble_effect_of_n_vp_trees}
\begin{tabular}{rrrrrr}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & n_vp_trees \\
\midrule
22392228 & 3893.338 & 0.714 & 1214.588 & 0.571 & 2 \\
33588342 & 5887.060 & 0.839 & 1487.469 & 0.700 & 3 \\
44784456 & 7892.651 & 0.881 & 1404.008 & 0.661 & 4 \\
55980570 & 9893.602 & 0.896 & 1331.287 & 0.630 & 5 \\
67176684 & 11887.347 & 0.904 & 1294.068 & 0.611 & 6 \\
78372798 & 13878.215 & 0.908 & 1266.866 & 0.600 & 7 \\
89568912 & 15865.055 & 0.906 & 1234.900 & 0.587 & 8 \\
100765026 & 17853.984 & 0.912 & 1215.717 & 0.578 & 9 \\
111961140 & 19843.889 & 0.911 & 1209.119 & 0.575 & 10 \\
123157254 & 21826.754 & 0.917 & 1194.160 & 0.569 & 11 \\
134353368 & 23811.094 & 0.919 & 1182.068 & 0.563 & 12 \\
145549482 & 25793.723 & 0.924 & 1175.655 & 0.560 & 13 \\
156745596 & 27779.021 & 0.926 & 1173.933 & 0.559 & 14 \\
167941710 & 29759.135 & 0.923 & 1166.601 & 0.555 & 15 \\
179137824 & 31740.627 & 0.926 & 1161.913 & 0.553 & 16 \\
190333938 & 33722.117 & 0.925 & 1151.812 & 0.549 & 17 \\
201530052 & 35701.660 & 0.921 & 1155.254 & 0.549 & 18 \\
212726166 & 37677.120 & 0.919 & 1145.443 & 0.545 & 19 \\
223922280 & 39663.170 & 0.919 & 1141.077 & 0.542 & 20 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=2, n=100000, build\_ndc=69840598, model=EnsembleParams, n\_vp\_trees=6, max\_chunk\_size=256, level\_norm=0.3, distance=Dot, strategy=BruteForceKNN, n\_candidates=0, k=30, ef=60, n\_queries=10000}
\label{exp_ensemble_effect_of_m_max}
\begin{tabular}{rrrrrrr}
\toprule
build_ms & recall & ndc & time_ms & same_chunk_m_max & m_max & m_max_0 \\
\midrule
11412.062 & 0.440 & 300.790 & 0.137 & 4 & 4 & 8 \\
11463.479 & 0.551 & 371.937 & 0.172 & 5 & 5 & 10 \\
11394.668 & 0.615 & 436.268 & 0.202 & 6 & 6 & 12 \\
11430.488 & 0.676 & 507.561 & 0.237 & 7 & 7 & 14 \\
11538.210 & 0.710 & 569.476 & 0.265 & 8 & 8 & 16 \\
11609.480 & 0.742 & 627.298 & 0.293 & 9 & 9 & 18 \\
11684.926 & 0.767 & 683.084 & 0.318 & 10 & 10 & 20 \\
11726.925 & 0.810 & 764.070 & 0.357 & 11 & 11 & 22 \\
11851.336 & 0.826 & 821.536 & 0.381 & 12 & 12 & 24 \\
11867.065 & 0.841 & 877.271 & 0.409 & 13 & 13 & 26 \\
11942.913 & 0.851 & 932.144 & 0.435 & 14 & 14 & 28 \\
11992.657 & 0.862 & 986.869 & 0.461 & 15 & 15 & 30 \\
12049.092 & 0.871 & 1040.416 & 0.483 & 16 & 16 & 32 \\
12084.646 & 0.878 & 1094.941 & 0.507 & 17 & 17 & 34 \\
12194.705 & 0.884 & 1146.181 & 0.526 & 18 & 18 & 36 \\
12250.056 & 0.890 & 1198.211 & 0.552 & 19 & 19 & 38 \\
12372.758 & 0.895 & 1250.633 & 0.574 & 20 & 20 & 40 \\
12427.707 & 0.900 & 1302.206 & 0.598 & 21 & 21 & 42 \\
12507.582 & 0.904 & 1354.145 & 0.622 & 22 & 22 & 44 \\
12562.799 & 0.908 & 1407.875 & 0.650 & 23 & 23 & 46 \\
12736.179 & 0.912 & 1455.491 & 0.672 & 24 & 24 & 48 \\
12785.533 & 0.914 & 1505.593 & 0.697 & 25 & 25 & 50 \\
12865.281 & 0.917 & 1554.663 & 0.719 & 26 & 26 & 52 \\
12911.572 & 0.919 & 1603.372 & 0.743 & 27 & 27 & 54 \\
12903.738 & 0.922 & 1654.607 & 0.767 & 28 & 28 & 56 \\
12959.842 & 0.924 & 1702.848 & 0.792 & 29 & 29 & 58 \\
13027.648 & 0.926 & 1751.111 & 0.810 & 30 & 30 & 60 \\
13103.729 & 0.928 & 1799.085 & 0.836 & 31 & 31 & 62 \\
13249.098 & 0.930 & 1850.020 & 0.858 & 32 & 32 & 64 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=EnsembleParams, n\_vp\_trees=6, same\_chunk\_m\_max=16, m\_max=20, m\_max\_0=40, level\_norm=0.0, distance=Dot, strategy=BruteForceKNN, n\_candidates=0, k=30, ef=60, n\_queries=1000}
\label{exp_ensemble_effect_of_chunk_size}
\begin{tabular}{rrrrrr}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & max_chunk_size \\
\midrule
23231916 & 4633.604 & 0.849 & 1463.603 & 0.704 & 64 \\
37880172 & 7033.389 & 0.885 & 1374.646 & 0.663 & 128 \\
67176684 & 11909.979 & 0.904 & 1294.068 & 0.626 & 256 \\
125770284 & 21123.740 & 0.914 & 1238.871 & 0.601 & 512 \\
125770284 & 21150.023 & 0.914 & 1238.871 & 0.602 & 768 \\
242957676 & 39286.080 & 0.924 & 1181.423 & 0.573 & 1024 \\
242957676 & 39230.754 & 0.924 & 1181.423 & 0.573 & 1280 \\
242957676 & 39242.310 & 0.924 & 1181.423 & 0.567 & 1536 \\
477332652 & 75316.570 & 0.925 & 1133.217 & 0.537 & 1792 \\
477332652 & 75278.960 & 0.925 & 1133.217 & 0.537 & 2048 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=6, n=100000, build\_ndc=85709152, model=Threaded EnsembleParams, n\_vp\_trees=8, max\_chunk\_size=256, m\_max=20, m\_max\_0=36, level\_norm=0.0, distance=Dot, strategy=BruteForceKNN, n\_candidates=0, k=30, ef=60, n\_queries=10000}
\label{exp_ensemble_effect_of_same_chunk_m_max}
\begin{tabular}{rrrrr}
\toprule
build_ms & recall & ndc & time_ms & same_chunk_m_max \\
\midrule
6259.573 & 0.907 & 1053.234 & 0.576 & 4 \\
6181.998 & 0.925 & 1246.996 & 0.749 & 5 \\
6182.699 & 0.927 & 1294.907 & 0.723 & 6 \\
6187.309 & 0.925 & 1278.940 & 0.707 & 7 \\
6161.070 & 0.923 & 1255.137 & 0.713 & 8 \\
6029.240 & 0.911 & 1211.618 & 0.659 & 12 \\
6083.416 & 0.905 & 1193.967 & 0.650 & 16 \\
6087.441 & 0.903 & 1193.839 & 0.647 & 20 \\
6037.885 & 0.901 & 1190.243 & 0.652 & 24 \\
6066.686 & 0.900 & 1189.221 & 0.652 & 28 \\
6087.686 & 0.901 & 1189.659 & 0.692 & 32 \\
6159.285 & 0.901 & 1189.634 & 0.648 & 36 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=EnsembleParams, n\_vp\_trees=6, max\_chunk\_size=256, same\_chunk\_m\_max=16, m\_max=20, m\_max\_0=40, distance=Dot, strategy=BruteForceKNN, n\_candidates=0, k=30, ef=60, n\_queries=100}
\label{exp_ensemble_effect_of_level_norm}
\begin{tabular}{rrrrrr}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & level_norm \\
\midrule
67176684 & 11983.370 & 0.904 & 1298.970 & 0.638 & 0.000 \\
67176684 & 11844.803 & 0.904 & 1298.970 & 0.633 & 0.050 \\
67176694 & 11831.283 & 0.893 & 1211.190 & 0.602 & 0.100 \\
67185330 & 11831.979 & 0.906 & 1279.460 & 0.629 & 0.150 \\
67582072 & 11894.173 & 0.899 & 1314.880 & 0.639 & 0.200 \\
68580208 & 12057.014 & 0.888 & 1264.260 & 0.613 & 0.250 \\
69840598 & 12256.352 & 0.906 & 1292.180 & 0.625 & 0.300 \\
70696168 & 12415.186 & 0.894 & 1292.390 & 0.617 & 0.350 \\
71308657 & 12527.430 & 0.894 & 1283.050 & 0.612 & 0.400 \\
74012118 & 12987.272 & 0.898 & 1265.780 & 0.602 & 0.450 \\
78254720 & 13691.054 & 0.895 & 1292.270 & 0.611 & 0.500 \\
82302120 & 14366.479 & 0.886 & 1325.870 & 0.624 & 0.550 \\
79988519 & 14013.448 & 0.904 & 1299.320 & 0.606 & 0.600 \\
82689681 & 14471.490 & 0.904 & 1314.690 & 0.613 & 0.650 \\
86909207 & 15180.615 & 0.889 & 1328.610 & 0.618 & 0.700 \\
92127998 & 16039.421 & 0.897 & 1318.270 & 0.608 & 0.750 \\
93988306 & 16364.342 & 0.888 & 1335.360 & 0.614 & 0.800 \\
99482533 & 17282.863 & 0.888 & 1357.310 & 0.625 & 0.850 \\
92119102 & 16120.451 & 0.899 & 1365.860 & 0.626 & 0.900 \\
95278398 & 16651.816 & 0.888 & 1349.140 & 0.614 & 0.950 \\
100148576 & 17482.188 & 0.891 & 1379.040 & 0.627 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: rep=1, n=100000, model=EnsembleParams, n\_vp\_trees=6, same\_chunk\_m\_max=20, m\_max=20, m\_max\_0=40, level\_norm=0.0, distance=Dot, n\_candidates=0, k=30, ef=60, n\_queries=10000}
\label{exp_ensemble_effect_of_brute_force_vs_rnn_smmax20}
\begin{tabular}{rrrrrrlrr}
\toprule
build_ndc & build_ms & recall & ndc & time_ms & max_chunk_size & strategy & o_loops & i_loops \\
\midrule
23231916 & 4800.201 & 0.843 & 1483.299 & 0.698 & 64 & BruteForceKNN & NaN & NaN \\
37880172 & 7256.031 & 0.877 & 1428.289 & 0.675 & 128 & BruteForceKNN & NaN & NaN \\
67176684 & 12094.769 & 0.899 & 1354.727 & 0.637 & 256 & BruteForceKNN & NaN & NaN \\
125770284 & 21325.332 & 0.915 & 1289.974 & 0.611 & 512 & BruteForceKNN & NaN & NaN \\
242957676 & 39444.363 & 0.922 & 1235.240 & 0.582 & 1024 & BruteForceKNN & NaN & NaN \\
477332652 & 75296.220 & 0.926 & 1178.463 & 0.555 & 2048 & BruteForceKNN & NaN & NaN \\
59612180 & 9652.037 & 0.832 & 1617.405 & 0.761 & 64 & RNNDescent & 1.000 & 3.000 \\
79538541 & 12764.426 & 0.877 & 1713.396 & 0.819 & 128 & RNNDescent & 1.000 & 3.000 \\
101553399 & 16005.511 & 0.906 & 1682.561 & 0.801 & 256 & RNNDescent & 1.000 & 3.000 \\
122153212 & 18965.281 & 0.927 & 1634.354 & 0.782 & 512 & RNNDescent & 1.000 & 3.000 \\
140312499 & 21539.310 & 0.938 & 1598.703 & 0.763 & 1024 & RNNDescent & 1.000 & 3.000 \\
154057684 & 23403.367 & 0.945 & 1580.320 & 0.752 & 2048 & RNNDescent & 1.000 & 3.000 \\
71070027 & 11728.043 & 0.843 & 1485.865 & 0.697 & 64 & RNNDescent & 2.000 & 3.000 \\
100194606 & 16342.523 & 0.891 & 1634.472 & 0.774 & 128 & RNNDescent & 2.000 & 3.000 \\
131850834 & 20992.822 & 0.924 & 1676.620 & 0.790 & 256 & RNNDescent & 2.000 & 3.000 \\
159773428 & 25216.752 & 0.947 & 1650.252 & 0.788 & 512 & RNNDescent & 2.000 & 3.000 \\
183637004 & 28661.178 & 0.960 & 1612.653 & 0.766 & 1024 & RNNDescent & 2.000 & 3.000 \\
202144994 & 31333.486 & 0.969 & 1554.420 & 0.739 & 2048 & RNNDescent & 2.000 & 3.000 \\
78178727 & 13090.421 & 0.843 & 1477.245 & 0.693 & 64 & RNNDescent & 3.000 & 3.000 \\
111522313 & 18442.176 & 0.891 & 1626.483 & 0.765 & 128 & RNNDescent & 3.000 & 3.000 \\
148452907 & 23907.855 & 0.924 & 1675.781 & 0.789 & 256 & RNNDescent & 3.000 & 3.000 \\
181616146 & 29066.686 & 0.947 & 1656.546 & 0.789 & 512 & RNNDescent & 3.000 & 3.000 \\
210496624 & 33300.016 & 0.962 & 1614.866 & 0.769 & 1024 & RNNDescent & 3.000 & 3.000 \\
233606763 & 36693.035 & 0.972 & 1570.894 & 0.745 & 2048 & RNNDescent & 3.000 & 3.000 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}
\caption{experiment: n=10120191, dims=768, k=30, start\_candidates=1, threaded=True}
\label{a_10m_multi_threaded}
\begin{tabular}{rrrrrrlrrrrrrrrrrl}
\toprule
ef & build_secs & build_ndc & search_ms & search_ndc & recall & model & level_norm & n_vp_trees & n_candidates & max_chunk_size & same_chunk_m_max & m_max & m_max0 & rnn_inner_loops & rnn_outer_loops & ef_constr & key \\
\midrule
30 & 586.675 & -654954792.000 & 0.928 & 1294.025 & 0.668 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
30 & 385.492 & 1393742960.000 & 1.511 & 2100.834 & 0.611 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
30 & 530.983 & NaN & 1.642 & NaN & 0.760 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
30 & 825.540 & -1100634707.000 & 0.667 & 833.549 & 0.771 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
30 & 456.338 & NaN & 0.308 & NaN & 0.788 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
30 & 481.951 & NaN & 11.576 & NaN & 0.807 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
40 & 586.675 & -654954792.000 & 1.100 & 1553.242 & 0.720 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
40 & 385.492 & 1393742960.000 & 1.782 & 2487.829 & 0.666 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
40 & 530.983 & NaN & 1.071 & NaN & 0.800 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
40 & 825.540 & -1100634707.000 & 0.781 & 980.770 & 0.818 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
40 & 456.338 & NaN & 0.454 & NaN & 0.825 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
40 & 481.951 & NaN & 11.298 & NaN & 0.841 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
50 & 586.675 & -654954792.000 & 1.307 & 1804.332 & 0.757 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
50 & 385.492 & 1393742960.000 & 2.087 & 2859.787 & 0.706 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
50 & 530.983 & NaN & 1.224 & NaN & 0.826 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
50 & 825.540 & -1100634707.000 & 0.912 & 1112.936 & 0.845 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
50 & 456.338 & NaN & 0.556 & NaN & 0.850 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
50 & 481.951 & NaN & 10.819 & NaN & 0.862 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
60 & 586.675 & -654954792.000 & 1.494 & 2049.017 & 0.784 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
60 & 385.492 & 1393742960.000 & 2.345 & 3218.124 & 0.737 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
60 & 530.983 & NaN & 1.350 & NaN & 0.844 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
60 & 825.540 & -1100634707.000 & 1.014 & 1236.116 & 0.865 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
60 & 456.338 & NaN & 0.667 & NaN & 0.867 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
60 & 481.951 & NaN & 11.010 & NaN & 0.878 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
70 & 586.675 & -654954792.000 & 1.689 & 2288.240 & 0.805 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
70 & 385.492 & 1393742960.000 & 2.651 & 3563.458 & 0.761 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
70 & 530.983 & NaN & 1.507 & NaN & 0.858 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
70 & 825.540 & -1100634707.000 & 1.257 & 1352.778 & 0.879 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
70 & 456.338 & NaN & 0.570 & NaN & 0.879 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
70 & 481.951 & NaN & 11.565 & NaN & 0.890 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
80 & 586.675 & -654954792.000 & 1.839 & 2524.181 & 0.823 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
80 & 385.492 & 1393742960.000 & 2.878 & 3901.627 & 0.781 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
80 & 530.983 & NaN & 2.177 & NaN & 0.869 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
80 & 825.540 & -1100634707.000 & 1.218 & 1465.683 & 0.889 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
80 & 456.338 & NaN & 0.515 & NaN & 0.888 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
80 & 481.951 & NaN & 11.893 & NaN & 0.898 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
90 & 586.675 & -654954792.000 & 2.051 & 2753.841 & 0.837 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
90 & 385.492 & 1393742960.000 & 3.163 & 4235.115 & 0.798 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
90 & 530.983 & NaN & 2.369 & NaN & 0.878 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
90 & 825.540 & -1100634707.000 & 1.327 & 1577.089 & 0.898 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
90 & 456.338 & NaN & 0.562 & NaN & 0.896 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
90 & 481.951 & NaN & 11.807 & NaN & 0.905 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
100 & 586.675 & -654954792.000 & 2.189 & 2982.158 & 0.849 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
100 & 385.492 & 1393742960.000 & 3.392 & 4561.934 & 0.812 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
100 & 530.983 & NaN & 2.371 & NaN & 0.884 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
100 & 825.540 & -1100634707.000 & 1.426 & 1687.277 & 0.905 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
100 & 456.338 & NaN & 0.606 & NaN & 0.903 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
100 & 481.951 & NaN & 11.548 & NaN & 0.910 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
110 & 586.675 & -654954792.000 & 2.362 & 3207.204 & 0.859 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
110 & 385.492 & 1393742960.000 & 3.651 & 4883.381 & 0.824 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
110 & 530.983 & NaN & 1.808 & NaN & 0.889 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
110 & 825.540 & -1100634707.000 & 1.510 & 1793.029 & 0.910 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
110 & 456.338 & NaN & 0.647 & NaN & 0.908 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
110 & 481.951 & NaN & 11.417 & NaN & 0.915 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
120 & 586.675 & -654954792.000 & 2.554 & 3430.464 & 0.868 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
120 & 385.492 & 1393742960.000 & 3.920 & 5193.780 & 0.834 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
120 & 530.983 & NaN & 1.939 & NaN & 0.894 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
120 & 825.540 & -1100634707.000 & 1.613 & 1899.028 & 0.915 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
120 & 456.338 & NaN & 0.694 & NaN & 0.913 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
120 & 481.951 & NaN & 11.224 & NaN & 0.919 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
130 & 586.675 & -654954792.000 & 2.751 & 3651.192 & 0.876 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
130 & 385.492 & 1393742960.000 & 4.248 & 5505.274 & 0.843 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
130 & 530.983 & NaN & 2.549 & NaN & 0.899 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
130 & 825.540 & -1100634707.000 & 1.732 & 2001.405 & 0.920 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
130 & 456.338 & NaN & 0.739 & NaN & 0.916 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
130 & 481.951 & NaN & 11.083 & NaN & 0.922 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
140 & 586.675 & -654954792.000 & 2.869 & 3871.666 & 0.883 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
140 & 385.492 & 1393742960.000 & 4.521 & 5811.866 & 0.852 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
140 & 530.983 & NaN & 2.335 & NaN & 0.903 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
140 & 825.540 & -1100634707.000 & 1.799 & 2103.191 & 0.923 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
140 & 456.338 & NaN & 0.780 & NaN & 0.919 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
140 & 481.951 & NaN & 12.455 & NaN & 0.925 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
150 & 586.675 & -654954792.000 & 3.092 & 4089.171 & 0.889 & Ensemble & 0.000 & 6.000 & 0.000 & 1024.000 & 10.000 & 40 & 40 & 3.000 & 2.000 & NaN & Ensemble 1024.0 6.0 \\
150 & 385.492 & 1393742960.000 & 4.636 & 6115.093 & 0.859 & Ensemble & 0.000 & 6.000 & 0.000 & 256.000 & 10.000 & 60 & 60 & NaN & NaN & NaN & Ensemble 256.0 6.0 \\
150 & 530.983 & NaN & 2.161 & NaN & 0.907 & Hnsw_jpboth & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_jpboth nan nan \\
150 & 825.540 & -1100634707.000 & 1.919 & 2203.314 & 0.926 & Hnsw_vecnn & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_vecnn nan nan \\
150 & 456.338 & NaN & 0.826 & NaN & 0.922 & Hnsw_hnswlib & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_hnswlib nan nan \\
150 & 481.951 & NaN & 10.515 & NaN & 0.928 & Hnsw_faiss & 0.300 & NaN & NaN & NaN & NaN & 20 & 40 & NaN & NaN & 60.000 & Hnsw_faiss nan nan \\
\bottomrule
\end{tabular}
\end{table}
